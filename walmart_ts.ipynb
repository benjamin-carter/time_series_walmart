{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "walmart ts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKw5vBUbbgOhYapGZLDrxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjamin-carter/time_series_walmart/blob/master/walmart_ts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGzNGAybCVO1",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 7: Forecasting Weekly Sales for Walmart Store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1AytvugCceT",
        "colab_type": "text"
      },
      "source": [
        "Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNK4nqdJI6o6",
        "colab_type": "code",
        "outputId": "ecc72df3-1f91-42fa-c6d5-07bb517a8d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXcTMP8WJzfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls \"/content/gdrive/My Drive/Current\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFlSh6GmCkpL",
        "colab_type": "text"
      },
      "source": [
        "Add necessary packages for data manipulation, dataframe handling, plotting,  time series modelling, and LSTM creation and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I83GlcEoJElg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "  \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "import datetime\n",
        "\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML, display_html\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8, 6)\n",
        "plt.rcParams['axes.grid'] = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_iialXJCzLp",
        "colab_type": "text"
      },
      "source": [
        "Upload Data from Google Drive. Upload and edit testing dataset (empty)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ymyDNker_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/Current/walmart_train.csv\")\n",
        "df_test = pd.read_csv(\"/content/gdrive/My Drive/Current/walmart_test.csv\")\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "df_example = df_by_store(df, 1)\n",
        "df_test_ = df_by_store(df_test, 1)\n",
        "df_test_ = df_test_['2012-11-02':'2013-02-01']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0PBh4IzDAk0",
        "colab_type": "text"
      },
      "source": [
        "# Decomposition\n",
        "Remove end-of-year pulse, overall trend, and saw-tooth function found in the original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7sEQIt7pbZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean10 = df_example['ts']['2010-02-11':'2010-11-18'].to_numpy().mean()\n",
        "mean11 = df_example['ts']['2011-02-05':'2011-11-12'].to_numpy().mean()\n",
        "temp10 = df_example['ts']['2010-11-25':'2011-02-03'].to_numpy()\n",
        "temp11 = df_example['ts']['2011-11-19':'2012-01-27'].to_numpy()\n",
        "\n",
        "temp10_p = temp10/mean10\n",
        "temp11_p = temp11/mean11\n",
        "\n",
        "temp_tot_p = .5*(temp10_p+temp11_p)\n",
        "\n",
        "df_example['ts']['2010-11-25':'2011-02-03'] = df_example['ts']['2010-11-25':'2011-02-03'].to_numpy() - mean10 * (temp_tot_p-1)\n",
        "df_example['ts']['2011-11-19':'2012-01-27'] = df_example['ts']['2011-11-19':'2012-01-27'].to_numpy() - mean11 * (temp_tot_p-1)\n",
        "\n",
        "Y = df_example['ts'].to_numpy().reshape(len(df_example['ts']),1)\n",
        "X = np.arange(len(Y)).reshape((len(Y),1))\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X,Y)\n",
        "growth = regressor.coef_\n",
        "init_int = regressor.intercept_\n",
        "df_example['ts_trend'] = regressor.intercept_ + X * regressor.coef_\n",
        "df_example['ts_trend_adj'] = df_example['ts'] - df_example['ts_trend']\n",
        "\n",
        "Y = df_example['ts_trend_adj'].to_numpy().reshape(len(df_example['ts']),1)\n",
        "X = np.arange(len(Y)).reshape((len(Y),1))\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X,Y)\n",
        "overall_trend = regressor.intercept_ + X * regressor.coef_\n",
        "\n",
        "len10 = len(df_example['ts_trend_adj']['2010-02-05':'2010-11-19'])\n",
        "Y = df_example['ts_trend_adj']['2010-02-05':'2010-11-19'].to_numpy().reshape(len10, 1)\n",
        "\n",
        "# len10 = len(df_example['ts_trend_adj']['2010-02-05':'2011-01-29'])\n",
        "# Y = df_example['ts_trend_adj']['2010-02-05':'2011-01-29'].to_numpy().reshape(len10, 1)\n",
        "X_10 = np.arange(len(Y)).reshape((len(Y),1))\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X_10,Y)\n",
        "rate_2010 = regressor.coef_\n",
        "int_2010 = regressor.intercept_\n",
        "\n",
        "len11 = len(df_example['ts_trend_adj']['2010-11-26':'2011-11-18'])\n",
        "Y = df_example['ts_trend_adj']['2010-11-26':'2011-11-18'].to_numpy().reshape(len11, 1)\n",
        "\n",
        "# len11 = len(df_example['ts_trend_adj']['2011-02-04':'2012-01-28'])\n",
        "# Y = df_example['ts_trend_adj']['2011-02-04':'2012-01-28'].to_numpy().reshape(len11, 1)\n",
        "X_11 = np.arange(len(Y)).reshape((len(Y),1))\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X_11,Y)\n",
        "rate_2011 = regressor.coef_\n",
        "int_2011 = regressor.intercept_\n",
        "\n",
        "len12 = len(df_example['ts_trend_adj']['2011-11-25':])\n",
        "Y = df_example['ts_trend_adj']['2011-11-25':].to_numpy().reshape(len12,1)\n",
        "\n",
        "# len12 = len(df_example['ts_trend_adj']['2012-02-03':])\n",
        "# Y = df_example['ts_trend_adj']['2012-02-03':].to_numpy().reshape(len12,1)\n",
        "X_12 = np.arange(len(Y)).reshape((len(Y),1))\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X_12,Y)\n",
        "rate_2012 = regressor.coef_\n",
        "int_2012 = regressor.intercept_\n",
        "\n",
        "overall_rate = (1/3) * (rate_2010 + rate_2011+ rate_2012)\n",
        "overall_int = (1/3) * (int_2010 + int_2011 + int_2012)\n",
        "trend_2010 = overall_int + X_10 * overall_rate\n",
        "trend_2011 = overall_int + X_11 * overall_rate\n",
        "trend_2012 = overall_int + X_12 * overall_rate\n",
        "\n",
        "saw_tooth = np.zeros((len(df_example),1))\n",
        "saw_tooth[:len10] = overall_trend[:len10] + trend_2010\n",
        "saw_tooth[len10:(len10 + len11)] = overall_trend[len10:(len10 + len11)] + trend_2011\n",
        "saw_tooth[(len10 + len11):] = overall_trend[(len10 + len11):] + trend_2012\n",
        "df_example['saw_tooth'] = saw_tooth\n",
        "df_example['ts_saw_adj'] = df_example['ts_trend_adj'] - df_example['saw_tooth']\n",
        "\n",
        "Y = df_example['ts_saw_adj'].to_numpy().reshape(len(df_example['ts_saw_adj']),1)\n",
        "X = np.arange(len(Y)).reshape((len(Y),1))\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X,Y)\n",
        "growth2 = regressor.coef_\n",
        "init_int2 = regressor.intercept_\n",
        "df_example['ts_st_trend'] = regressor.intercept_ + X * regressor.coef_\n",
        "df_example['ts_trend_adj_v2'] = df_example['ts_saw_adj'] - df_example['ts_st_trend']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRXPyzbGDLej",
        "colab_type": "text"
      },
      "source": [
        "## Create plot for each decomposed function to illustrate process and result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y5DhNFGKOr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_example['ts_moving_avg'] = df_example['ts'].rolling(window = 7, center= False).mean()\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df_example['ts'], linewidth = 3)\n",
        "plt.title(\"Store #1: Weekly Sales\", fontsize = 20)\n",
        "plt.xlabel(\"Date\", fontsize = 20)\n",
        "plt.ylabel(\"$M\", fontsize = 20)\n",
        "plt.show()\n",
        "plt.clf()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df_example['ts']['2010-11-18':'2011-02-10'], linewidth = 3)\n",
        "plt.plot(df_example['ts']['2011-11-12':'2012-02-04'], linewidth = 3)\n",
        "plt.title(\"Nov-Jan Pulse\", fontsize = 20)\n",
        "plt.xlabel(\"Date\", fontsize = 20)\n",
        "plt.ylabel(\"$M\", fontsize = 20)\n",
        "plt.show()\n",
        "plt.clf()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Upward Trend\", fontsize = 20)\n",
        "plt.xlabel(\"Date\", fontsize = 20)\n",
        "plt.ylabel(\"$M\", fontsize = 20)\n",
        "plt.plot(df_example['ts']['2010-02-05':'2011-01-28'])\n",
        "plt.plot(df_example['ts']['2011-02-04':'2012-01-27'])\n",
        "plt.plot(df_example['ts']['2012-02-03':'2012-10-26'])\n",
        "plt.plot(df_example['ts_trend'])\n",
        "plt.show()\n",
        "plt.clf()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Annual Decrease\", fontsize = 20)\n",
        "plt.xlabel(\"Date\", fontsize = 20)\n",
        "# plt.ylabel(\"$M\", fontsize = 20)\n",
        "plt.plot(df_example['ts_trend_adj'])\n",
        "plt.plot(df_example['saw_tooth'])\n",
        "plt.show()\n",
        "plt.clf()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Final Noise Signal\", fontsize = 20)\n",
        "plt.xlabel(\"Date\", fontsize = 20)\n",
        "# plt.ylabel(\"$M\", fontsize = 20)\n",
        "plt.plot(df_example['ts_trend_adj_v2'])\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiSBIOKPD-v_",
        "colab_type": "text"
      },
      "source": [
        "## Time Series Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTu_HIg6DSyr",
        "colab_type": "text"
      },
      "source": [
        "Check stationarity for final signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArbuB1nsKNvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_stationarity(df_example, 'ts_trend_adj_v2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-q6w1lJDV7D",
        "colab_type": "text"
      },
      "source": [
        "Run decomposition process (unnecessary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg0XX0iDR6Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decomposition = seasonal_decompose(df_example['ts'], freq = 52)\n",
        "\n",
        "df_example.loc[:,'trend'] = decomposition.trend\n",
        "df_example.loc[:,'seasonal'] = decomposition.seasonal\n",
        "df_example.loc[:,'residual'] = decomposition.resid\n",
        "\n",
        "plot_decomposition(df = df_example, ts= 'ts',trend='trend', seasonal='seasonal',residual='residual')\n",
        "\n",
        "test_stationarity(df_example.dropna(), ts='residual')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5QjgMD_DhBc",
        "colab_type": "text"
      },
      "source": [
        "Run ACF and PACF analysis. Plot results with bar graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcyFSKtFoi94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lag_acf=acf(np.array(df_example['ts_trend_adj_v2']),nlags=10)\n",
        "lag_pacf=pacf(np.array(df_example['ts_trend_adj_v2']),nlags=10)\n",
        "\n",
        "lag_acf = pd.DataFrame(lag_acf)\n",
        "lag_acf.plot(kind = 'bar')\n",
        "plt.title(\"ACF\", fontsize = 20)\n",
        "plt.axhline(y=0,linestyle='--',color='gray')\n",
        "plt.axhline(y=-1.96/np.sqrt(143),linestyle='--',color='gray')\n",
        "plt.axhline(y=1.96/np.sqrt(143),linestyle='--',color='gray')\n",
        "lag_pacf = pd.DataFrame(lag_pacf)\n",
        "lag_pacf.plot(kind = 'bar')\n",
        "plt.title(\"PACF\", fontsize = 20)\n",
        "plt.axhline(y=0,linestyle='--',color='gray')\n",
        "plt.axhline(y=-1.96/np.sqrt(143),linestyle='--',color='gray')\n",
        "plt.axhline(y=1.96/np.sqrt(143),linestyle='--',color='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrRVuH_kDoL2",
        "colab_type": "text"
      },
      "source": [
        "Run ARIMA model with (p,d,q) found in analysis. Create forecast for 14 weeks ahead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB_mNsOdeyAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=ARIMA(df_example['ts_trend_adj_v2'], order=(4,0,5))\n",
        "results_arima=model.fit(disp=-1)\n",
        "decomp_last12 = results_arima.fittedvalues['2012-08-10':]\n",
        "arima_pred = results_arima.predict(start = '2012-11-02', end = '2013-02-01')\n",
        "arima_pred = arima_pred.to_numpy()\n",
        "conf_int = results_arima.forecast(14,alpha=.05)[2]\n",
        "# full_pred = np.column_stack((arima_pred, conf_int))\n",
        "# plt.plot(full_pred[:,0], label = \"Prediction\")\n",
        "# plt.plot(full_pred[:,1], label = \"95% Lower bound\")\n",
        "# plt.plot(full_pred[:,2], label = \"95% Upper bound\")\n",
        "# plt.legend(loc='lower left')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGEPqwOEDvhW",
        "colab_type": "text"
      },
      "source": [
        "### Backwards Engineer results from signal analysis to be comparable with original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLCQ2wJ26Ofl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arima_pred = arima_pred.reshape(len(arima_pred),)\n",
        "df_example['ts_last12_decomp'] = df_example['ts']\n",
        "df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] = decomp_last12.to_numpy()\n",
        "Y = df_example['ts_saw_adj'].to_numpy().reshape(len(df_example['ts_saw_adj']),1)\n",
        "X = np.arange(len(Y)).reshape((len(Y),1))\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X,Y)\n",
        "X_test = np.arange(len(Y),len(Y)+ 14).reshape(14,)\n",
        "temp =  regressor.intercept_ + X_test * regressor.coef_ + arima_pred\n",
        "temp = temp.reshape(14,)\n",
        "df_test_['ts_bef_saw'] = temp\n",
        "temp =  regressor.intercept_ + X_test * regressor.coef_ + conf_int[:,0]\n",
        "temp = temp.reshape(14,)\n",
        "df_test_['ts_bef_saw_down'] = temp\n",
        "temp =  regressor.intercept_ + X_test * regressor.coef_ + conf_int[:,1]\n",
        "temp = temp.reshape(14,)\n",
        "df_test_['ts_bef_saw_up'] = temp\n",
        "temp =  regressor.intercept_ + X[-12:] * regressor.coef_\n",
        "temp = temp.reshape(12,)\n",
        "df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] = df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] + temp\n",
        "\n",
        "Y = df_example['ts_trend_adj'].to_numpy().reshape(len(df_example['ts']),1)\n",
        "X = np.arange(len(Y)).reshape((len(Y),1))\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X,Y)\n",
        "X_test = np.arange(len12,len12+ 14).reshape(14,1)\n",
        "future_14 = overall_int + X_test * overall_rate\n",
        "future_12 = overall_int + X[-12:] * overall_rate\n",
        "overall_future = regressor.intercept_ + X_test * regressor.coef_\n",
        "overall_future12 = regressor.intercept_ + X[-12:] * regressor.coef_\n",
        "df_test_['ts_bef_OT'] = df_test_['ts_bef_saw'] + future_14.reshape(14,)\n",
        "df_test_['ts_bef_trend'] = df_test_['ts_bef_OT'] + overall_future.reshape(14,)\n",
        "df_test_['ts_bef_OT_down'] = df_test_['ts_bef_saw_down'] + future_14.reshape(14,)\n",
        "df_test_['ts_bef_OT_up'] = df_test_['ts_bef_saw_up'] + future_14.reshape(14,)\n",
        "df_test_['ts_bef_trend_down'] = df_test_['ts_bef_OT_down'] + overall_future.reshape(14,)\n",
        "df_test_['ts_bef_trend_up'] = df_test_['ts_bef_OT_up'] + overall_future.reshape(14,)\n",
        "df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] = df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] + future_12.reshape(12,)\n",
        "df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] = df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] + overall_future12.reshape(12,)\n",
        "\n",
        "Y = df_example['ts'].to_numpy().reshape(len(df_example['ts']),1)\n",
        "X = np.arange(len(Y)).reshape((len(Y),1))\n",
        "X_test = np.arange(len(Y),len(Y)+ 14).reshape(14,)\n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(X,Y)\n",
        "temp = regressor.intercept_ + X_test * regressor.coef_\n",
        "temp = temp.reshape(14,)\n",
        "df_test_['ts_bef_pulse'] = df_test_['ts_bef_trend'] + temp\n",
        "df_test_['ts_bef_pulse_down'] = df_test_['ts_bef_trend_down'] + temp\n",
        "df_test_['ts_bef_pulse_up'] = df_test_['ts_bef_trend_up'] + temp\n",
        "temp = regressor.intercept_ + X[-12:] * regressor.coef_\n",
        "temp = temp.reshape(12,)\n",
        "df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] = df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] + temp\n",
        "\n",
        "mean12 = df_example['ts']['2012-02-04':'2012-10-26'].to_numpy().mean()\n",
        "temp_tot_p = .5*(temp10_p+temp11_p)\n",
        "df_test_['ts_predict'] = df_test_['ts_bef_pulse']\n",
        "df_test_['ts_predict']['2012-11-23':'2013-01-25'] = df_test_['ts_predict']['2012-11-23':'2013-01-25'] + mean12 * (temp_tot_p-1)\n",
        "df_test_['ts_predict_down'] = df_test_['ts_bef_pulse_down']\n",
        "df_test_['ts_predict_down']['2012-11-23':'2013-01-25'] = df_test_['ts_predict_down']['2012-11-23':'2013-01-25'] + mean12 * (temp_tot_p-1)\n",
        "df_test_['ts_predict_up'] = df_test_['ts_bef_pulse_up']\n",
        "df_test_['ts_predict_up']['2012-11-23':'2013-01-25'] = df_test_['ts_predict_up']['2012-11-23':'2013-01-25'] + mean12 * (temp_tot_p-1)\n",
        "\n",
        "# df_test_['ts_predict_down'] = df_test_['ts_predict_down'] - df_example['ts'].std()\n",
        "# df_test_['ts_predict_up'] = df_test_['ts_predict_up'] + df_example['ts'].std()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNhl01S0ECsq",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzHWFkCzEH8_",
        "colab_type": "text"
      },
      "source": [
        "Download a fresh batch of the same data, in order to compare the results more easily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cTCFOBJJuMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/Current/walmart_train.csv\")\n",
        "df_test = pd.read_csv(\"/content/gdrive/My Drive/Current/walmart_test.csv\")\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "df_example2 = df_by_store(df, 1)\n",
        "df_test_2 = df_by_store(df_test, 1)\n",
        "df_test_2 = df_test_['2012-11-02':'2013-02-01']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv4O_aJYEO-r",
        "colab_type": "text"
      },
      "source": [
        "Create LSTM data and model specifically for the 14 week blind forecast and analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssqsIn5kIFJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_np = df_example2['ts'].to_numpy()\n",
        "ts_mean = ts_np.mean()\n",
        "ts_std = ts_np.std()\n",
        "ts_np = (ts_np - ts_mean) / ts_std\n",
        "zero_14 = np.zeros((1,15))\n",
        "ts_np = np.append(ts_np,zero_14)\n",
        "ts_ord, ts_test = univariate_data(ts_np, start_index=0, end_index=None, history_size = 50, target_size = 14)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_univariate = tf.data.Dataset.from_tensor_slices((ts_ord[:-15], ts_test[:-15]))\n",
        "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_univariate = tf.data.Dataset.from_tensor_slices((ts_ord[-15:-14], ts_test[-15:-14]))\n",
        "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "future_univ = tf.data.Dataset.from_tensor_slices((ts_ord[-1:], ts_test[-1:]))\n",
        "future_univ = future_univ.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "simple_lstm_model_f = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(32, return_sequences = True, input_shape=ts_ord.shape[-2:]),\n",
        "    tf.keras.layers.LSTM(16, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(14)\n",
        "])\n",
        "\n",
        "simple_lstm_model_f.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "EVALUATION_INTERVAL = 100\n",
        "EPOCHS = 10\n",
        "\n",
        "simple_lstm_model_f.fit(train_univariate, epochs=EPOCHS,\n",
        "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=val_univariate, validation_steps=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd7YtDW5EjA1",
        "colab_type": "text"
      },
      "source": [
        "Run results and calculate metrics for the 14 week blind forecast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrJyW3Yzi2SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x, y in future_univ.take(1):\n",
        "  lstm_fc14 = simple_lstm_model_f.predict(x)[0]\n",
        "lstm_fc14 = lstm_fc14.reshape(14,1)\n",
        "lstm_fc14 = lstm_fc14 *ts_std + ts_mean\n",
        "x, y = next(iter(future_univ))\n",
        "\n",
        "for x, y in future_univ.take(1):\n",
        "  plot = show_plot([x[0].numpy(),simple_lstm_model_f.predict(x)[0]], 0, 'Simple LSTM model')\n",
        "  plot.show()\n",
        "\n",
        "df_test_2['LSTM_fc'] =lstm_fc14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h2wpnIfEUtT",
        "colab_type": "text"
      },
      "source": [
        "Create LSTM data and model specifcally for the 12 week known forecast and analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sElX-xwtmgnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_np = df_example2['ts'].to_numpy()\n",
        "ts_mean = ts_np.mean()\n",
        "ts_std = ts_np.std()\n",
        "ts_np = (ts_np - ts_mean) / ts_std\n",
        "ts_ord, ts_test = univariate_data(ts_np, start_index=0, end_index=None, history_size = 52, target_size = 12)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_univariate = tf.data.Dataset.from_tensor_slices((ts_ord[:-1], ts_test[:-1]))\n",
        "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_univariate_12 = tf.data.Dataset.from_tensor_slices((ts_ord[-1:], ts_test[-1:]))\n",
        "val_univariate_12 = val_univariate_12.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "simple_lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(32, return_sequences = True, input_shape=ts_ord.shape[-2:]),\n",
        "    tf.keras.layers.LSTM(16, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(12)\n",
        "])\n",
        "\n",
        "simple_lstm_model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "EVALUATION_INTERVAL = 100\n",
        "EPOCHS = 10\n",
        "\n",
        "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
        "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=val_univariate_12, validation_steps=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIJJZ-GDEzED",
        "colab_type": "text"
      },
      "source": [
        "Run results and calculate metrics for the 12 week known forecast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhdzfulTkXMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x, y in val_univariate_12.take(1):\n",
        "  lstm_fc = simple_lstm_model.predict(x)[0]\n",
        "lstm_fc = lstm_fc.reshape(12,1)\n",
        "lstm_fc = lstm_fc *ts_std + ts_mean\n",
        "x, y = next(iter(val_univariate_12))\n",
        "y = np.array(y).reshape(12,1)\n",
        "y = y *ts_std + ts_mean\n",
        "lstm_fc = lstm_fc.reshape(12,)\n",
        "df_example2['ts_last12_lstm'] = df_example['ts']\n",
        "df_example2['ts_last12_lstm']['2012-08-10':'2012-10-26'] = lstm_fc.reshape(12,)\n",
        "lstm_fc = lstm_fc.reshape(12,1)\n",
        "rmse = np.sqrt(sum((lstm_fc-y)**2))\n",
        "rmse = np.asscalar(rmse)\n",
        "rmse = round(rmse)\n",
        "\n",
        "for x, y in val_univariate_12.take(1):\n",
        "  plot = show_plot([x[0].numpy(),  simple_lstm_model.predict(x)[0], y[0].numpy()],\n",
        "                    0, 'Simple LSTM model')\n",
        "  plot.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE50W1Z7FgOl",
        "colab_type": "text"
      },
      "source": [
        "## Attempt to train single model with different groups of stores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dft2OlJh3Fdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model before\n",
        "simple_lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(64, return_sequences = True, input_shape=(104,26)),\n",
        "    tf.keras.layers.LSTM(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(26)\n",
        "])\n",
        "simple_lstm_model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "all_group = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45]\n",
        "\n",
        "size_group = [14,41,39,34,40,8,26,21,25,10,9,15,18,22,45,23,12,35,29,1]\n",
        "typeA_group = [13,11,28,4,27,19,24,31,20,32,6,2,14,41,39,34,40,8,26,1]\n",
        "dept_group = [2,4,6,13,34,40,1]\n",
        "all_groups = []\n",
        "all_groups.append(all_group)\n",
        "all_groups.append(size_group)\n",
        "all_groups.append(typeA_group)\n",
        "all_groups.append(dept_group)\n",
        "for j in range(len(all_groups)):\n",
        "  for i in range(len(all_groups[j])):\n",
        "    df_store = df_by_store(df, all_groups[j][i])\n",
        "    ts_np = df_example['ts'].to_numpy()\n",
        "    days = len(ts_np)\n",
        "    # Standardize data\n",
        "    ts_np_norm = (ts_np - ts_np.mean()) / ts_np.std()\n",
        "    # Break up windows fro training and testing\n",
        "    ts_ord, ts_test = univariate_data(ts_np_norm, start_index=0, end_index=None, history_size = 104 , target_size = 26)\n",
        "    #Create tensors for training and validation\n",
        "    BATCH_SIZE = 64\n",
        "    BUFFER_SIZE = 10000\n",
        "    train_univariate = tf.data.Dataset.from_tensor_slices((ts_ord[:-1], ts_test[:-1]))\n",
        "    train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "    val_univariate = tf.data.Dataset.from_tensor_slices((ts_ord[-1:], ts_test[-1:]))\n",
        "    val_univariate = val_univariate.batch(BATCH_SIZE).repeat()\n",
        "    # Train model\n",
        "    EVALUATION_INTERVAL = 100 \n",
        "    EPOCHS = 10\n",
        "    simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
        "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=val_univariate, validation_steps=52)\n",
        "\n",
        "df_store = df_by_store(df, 1)\n",
        "ts_np = df_example['ts'].to_numpy()\n",
        "days = len(ts_np)\n",
        "# Standardize data\n",
        "ts_np_norm = (ts_np - ts_np.mean()) / ts_np.std()\n",
        "# Break up windows fro training and testing\n",
        "ts_ord, ts_test = univariate_data(ts_np_norm, start_index=0, end_index=None, history_size = 104, target_size = 26)\n",
        "#Create tensors for training and validation\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "train_univariate = tf.data.Dataset.from_tensor_slices((ts_ord[:-1], ts_test[:-1]))\n",
        "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "val_univariate = tf.data.Dataset.from_tensor_slices((ts_ord[-1:], ts_test[-1:]))\n",
        "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()\n",
        "# Train model\n",
        "EVALUATION_INTERVAL = 100 \n",
        "EPOCHS = 10\n",
        "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
        "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=val_univariate, validation_steps=12)\n",
        "# Predict sales \n",
        "for x, y in val_univariate.take(1):\n",
        "  sales_pred = simple_lstm_model.predict(x)[0]\n",
        "# Convert back into true $ units\n",
        "sales_pred = sales_pred * ts_np.std() + ts_np.mean()\n",
        "\n",
        "df_example['ts_last12_all'] = df_example['ts']\n",
        "df_example['ts_last12_all']['2012-08-10':'2012-10-26']  = sales_pred[-12:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpMxX4IAFtzJ",
        "colab_type": "text"
      },
      "source": [
        "Create datasets and running them through the model for 14 blind forecast for the grouped store model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64rWoJMMaju3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts_np = df_example2['ts'].to_numpy()\n",
        "ts_mean = ts_np.mean()\n",
        "ts_std = ts_np.std()\n",
        "ts_np = (ts_np - ts_mean) / ts_std\n",
        "zero_27 = np.zeros((1,27))\n",
        "ts_np = np.append(ts_np,zero_27)\n",
        "ts_ord, ts_test = univariate_data(ts_np, start_index=0, end_index=None, history_size = 104, target_size = 26)\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "future_univ_all = tf.data.Dataset.from_tensor_slices((ts_ord[-1:], ts_test[-1:]))\n",
        "future_univ_all = future_univ_all.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "for x, y in future_univ_all.take(1):\n",
        "  temp = simple_lstm_model_all.predict(x)[0]\n",
        "temp = temp * ts_std + ts_mean\n",
        "df_test_['ts_pred_all'] = temp[:14]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjiAUeNFGBbm",
        "colab_type": "text"
      },
      "source": [
        "# Prophet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpyjUuqfGEkx",
        "colab_type": "text"
      },
      "source": [
        "Adding necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enyGn2Qfn3mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fbprophet\n",
        "!pip install plotly\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from fbprophet import Prophet\n",
        "from fbprophet.plot import plot_plotly\n",
        "from fbprophet.plot import add_changepoints_to_plot\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as seabornInstance\n",
        "import plotly.offline as py\n",
        " \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "py.init_notebook_mode()\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFtHdFjhGHlx",
        "colab_type": "text"
      },
      "source": [
        "Creating dataset that was compatible with Prophet functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-a5x766oEJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_prophet = df_by_store_prophet(df, 1)\n",
        "# df_prophet['ds'] = df_prophet.index\n",
        "df_prophet['ds'] = df_prophet.index\n",
        "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
        "df_prophet.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWYR2HguGMH5",
        "colab_type": "text"
      },
      "source": [
        "Run Prophet modelling and get results for both y-fitted values and y-hat future values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAAvf1oan4Q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prophet = Prophet()\n",
        "prophet.fit(df_prophet)\n",
        "future = prophet.make_future_dataframe(periods=14, freq='W')\n",
        "forecast = prophet.predict(future)\n",
        "fig = prophet.plot(forecast)\n",
        "add_changepoints_to_plot(fig.gca(), prophet, forecast)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDdRyakeGTNw",
        "colab_type": "text"
      },
      "source": [
        "Store values to use in comparison with other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mm69ntisJs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_pro_14 = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']][-14:]\n",
        "temp_pro_14np = temp_pro_14['yhat'].to_numpy()\n",
        "df_test_['ts_proph'] = temp_pro_14np\n",
        "\n",
        "temp_pro_12 = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']][-26:-14]\n",
        "temp_pro_12np = temp_pro_12['yhat'].to_numpy()\n",
        "df_example['ts_prophet'] = df_example['ts']\n",
        "df_example['ts_prophet']['2012-08-10':'2012-10-26'] = temp_pro_12np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ku_p_CmE6jU",
        "colab_type": "text"
      },
      "source": [
        "## Plot different comparisons of different model's results and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7009xd0dJGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(df_test_['ts_predict'], label = \"Decomposition\")\n",
        "# plt.plot(df_test_['ts_predict_up'], label = \"95% Upper bound\")\n",
        "# plt.plot(df_test_['ts_predict_down'], label = \"95% Lower bound\")\n",
        "plt.plot(df_test_2['LSTM_fc'], label = \"LSTM - Store 1\")\n",
        "# plt.plot(df_test_['ts_pred_all'], label = \"LSTM - All\" )\n",
        "plt.plot(df_test_['ts_proph'], label = \"Prophet\")\n",
        "plt.title(\"14 Week Blind Forecast\")\n",
        "plt.legend(loc='top right')\n",
        "plt.show()\n",
        "plt.clf()\n",
        "\n",
        "decomp = df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'].to_numpy()\n",
        "lstm_rm = df_example2['ts_last12_lstm']['2012-08-10':'2012-10-26'].to_numpy()\n",
        "actual_rm = df_example['ts']['2012-08-10':'2012-10-26'].to_numpy()\n",
        "\n",
        "decomp_rmse = np.sqrt(sum((decomp-actual_rm)**2))\n",
        "lstm_rmse = np.sqrt(sum((lstm_rm-actual_rm)**2))\n",
        "decomp_rmse =  round(decomp_rmse)\n",
        "lstm_rmse = round(lstm_rmse)\n",
        "lstm_all = sales_pred[-12:]\n",
        "lstmall_rmse = np.sqrt(sum((lstm_all-actual_rm)**2))\n",
        "lstmall_rmse = round(lstmall_rmse)\n",
        "temp_pro_np\n",
        "proph_rmse = np.sqrt(sum((temp_pro_12np-actual_rm)**2))\n",
        "proph_rmse = round(proph_rmse)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(df_example['ts']['2012-08-10':'2012-10-26'], label = \"Actual\")\n",
        "# plt.plot(df_example['ts_last12_decomp']['2012-08-10':'2012-10-26'] , label = \"Decomp. RMSE: %s\" %decomp_rmse)\n",
        "plt.plot(df_example2['ts_last12_lstm']['2012-08-10':'2012-10-26'], label = \"LSTM - Store1. RMSE: %s\" %lstm_rmse)\n",
        "# plt.plot(df_example['ts_last12_all']['2012-08-10':'2012-10-26'], label = \"LSTM - All. RMSE: %s\" %lstmall_rmse)\n",
        "plt.plot(df_example['ts_prophet']['2012-08-10':'2012-10-26'], label = \"Prophet. RMSE: %s\" %proph_rmse)\n",
        "plt.title(\"12 Week Known Forecast\")\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n",
        "plt.clf()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIy2nASFFQzd",
        "colab_type": "text"
      },
      "source": [
        "## Global Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOftpOWKRe7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_by_store(df, store_no):\n",
        "  df_store = df[df['Store'] == store_no]\n",
        "  df_store = df_store.rename(columns = {'Date': 'ds', 'Weekly_Sales':'ts'})\n",
        "  df_example = df_store.groupby(by = 'ds').agg({'ts':'sum'})\n",
        "  df_example.index = pd.to_datetime(df_example.index) \n",
        "  df_example = df_example.asfreq(freq='7D')\n",
        "  df_example = df_example.sort_index(ascending = True)\n",
        "  df_example = df_example.fillna(value=0)\n",
        "  return df_example\n",
        "\n",
        "def df_by_dept(df, dept_no):\n",
        "  df_store = df[df['Dept'] == dept_no]\n",
        "  df_store = df_store.rename(columns = {'Date': 'ds', 'Weekly_Sales':'ts'})\n",
        "  df_example = df_store.groupby(by = 'ds').agg({'ts':'mean'})\n",
        "  df_example.index = pd.to_datetime(df_example.index) \n",
        "  df_example = df_example.asfreq(freq='7D')\n",
        "  df_example = df_example.sort_index(ascending = True)\n",
        "  df_example = df_example.fillna(value=0)\n",
        "  return df_example\n",
        "\n",
        "\n",
        "def test_stationarity(df, ts):\n",
        "    \"\"\"\n",
        "    Test stationarity using moving average statistics and Dickey-Fuller test\n",
        "    Source: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
        "    \"\"\"\n",
        "    \n",
        "    # Determing rolling statistics\n",
        "    rolmean = df[ts].rolling(window = 12, center = False).mean()\n",
        "    rolstd = df[ts].rolling(window = 12, center = False).std()\n",
        "    \n",
        "    # Plot rolling statistics:\n",
        "    orig = plt.plot(df[ts], \n",
        "                    color = 'blue', \n",
        "                    label = 'Original')\n",
        "    mean = plt.plot(rolmean, \n",
        "                    color = 'red', \n",
        "                    label = 'Rolling Mean')\n",
        "    std = plt.plot(rolstd, \n",
        "                   color = 'black', \n",
        "                   label = 'Rolling Std')\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.title('Rolling Mean & Standard Deviation for %s' %(ts), fontsize = 15)\n",
        "    plt.xticks(rotation = 45)\n",
        "    plt.show(block = False)\n",
        "    plt.close()\n",
        "    \n",
        "    # Perform Dickey-Fuller test:\n",
        "    # Null Hypothesis (H_0): time series is not stationary\n",
        "    # Alternate Hypothesis (H_1): time series is stationary\n",
        "    print ('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(df[ts], \n",
        "                      autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], \n",
        "                         index = ['Test Statistic',\n",
        "                                  'p-value',\n",
        "                                  '# Lags Used',\n",
        "                                  'Number of Observations Used'])\n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print (dfoutput)\n",
        "\n",
        "def plot_decomposition(df, ts,trend,seasonal, residual):\n",
        "\n",
        "  f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize = (15, 5), sharex = True)\n",
        "\n",
        "  ax1.plot(df[ts], label = 'Original')\n",
        "  ax1.legend(loc = 'best')\n",
        "  ax1.tick_params(axis = 'x', rotation = 45)\n",
        "\n",
        "  ax2.plot(df[trend], label = 'Trend')\n",
        "  ax2.legend(loc = 'best')\n",
        "  ax2.tick_params(axis = 'x', rotation = 45)\n",
        "\n",
        "  ax3.plot(df[seasonal],label = 'Seasonality')\n",
        "  ax3.legend(loc = 'best')\n",
        "  ax3.tick_params(axis = 'x', rotation = 45)\n",
        "\n",
        "  ax4.plot(df[residual], label = 'Residuals')\n",
        "  ax4.legend(loc = 'best')\n",
        "  ax4.tick_params(axis = 'x', rotation = 45)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.subtitle('Signal Decomposition of  %s' %(ts), x =0.5, y= 1.05, fontsize = 18)\n",
        "  plt.show()\n",
        "\n",
        "def plot_acf_pacf(df, ts):\n",
        "  \"\"\"\n",
        "  Plot auto-correlation function (ACF) and partial auto-correlation (PACF) plots\n",
        "  \"\"\"\n",
        "  f, (ax1, ax2) = plt.subplots(1,2, figsize = (10, 5)) \n",
        "\n",
        "  #Plot ACF: \n",
        "\n",
        "  ax1.plot(lag_acf)\n",
        "  ax1.axhline(y=0,linestyle='--',color='gray')\n",
        "  ax1.axhline(y=-1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
        "  ax1.axhline(y=1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
        "  ax1.set_title('Autocorrelation Function for %s' %(ts))\n",
        "\n",
        "  #Plot PACF:\n",
        "  ax2.plot(lag_pacf)\n",
        "  ax2.axhline(y=0,linestyle='--',color='gray')\n",
        "  ax2.axhline(y=-1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
        "  ax2.axhline(y=1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
        "  ax2.set_title('Partial Autocorrelation Function for %s' %(ts))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "  return\n",
        "\n",
        "def run_arima(df,ts,p,d,q):\n",
        "  \n",
        "  model=ARIMA(df[ts], order=(p,d,q))\n",
        "  results_arima=model.fit(disp=-1)\n",
        "\n",
        "  len_results = len(results_arima.fittedvalues)\n",
        "  ts_modified = df[ts][-len_results:]\n",
        "\n",
        "  rss  = sum((results_arima.fittedvalues - ts_modified)**2)\n",
        "  rmse = np.sqrt(rss/ len(df[ts]))\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(df[ts], label = \"Actual\")\n",
        "  plt.plot(results_arima.fittedvalues, color='red', label = \"Fitted\")\n",
        "  plt.legend(loc = 'best')\n",
        "  plt.title('RMSE: %.4f'% rmse, fontsize=20)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return results_arima\n",
        "\n",
        "def days_between(d1, d2):\n",
        "  d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
        "  d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
        "  return abs((d2-d1).days + 1)\n",
        "\n",
        "def daily_forecast (df, holidays, growth, holidays_prior_scale = 10, n_changepoints = 25, changepoint_prior_scale = 0.05, \n",
        "                   changepoints = None, interval_width = 0.8, mcmc_samples = 1, future_num_points = 10, daily_seasonality = True):\n",
        "  df_c = df.copy()\n",
        "  m = Prophet(growth = growth,\n",
        "              n_changepoints = n_changepoints,\n",
        "              changepoint_prior_scale = changepoint_prior_scale,\n",
        "              changepoints = changepoints,\n",
        "              holidays = holidays,\n",
        "              holidays_prior_scale = holidays_prior_scale,\n",
        "              interval_width = interval_width,\n",
        "              mcmc_samples = mcmc_samples, \n",
        "              daily_seasonality = daily_seasonality)\n",
        "  m.fit(df_c)\n",
        "  future = m.make_future_dataframe(periods = future_num_points)\n",
        "  forecast_1 = m.predict(future)\n",
        "\n",
        "  m.plot(forecast_1);\n",
        "  m.plot_components(forecast_1)\n",
        "\n",
        "  return forecast_1\n",
        "\n",
        "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i)\n",
        "    # Reshape data from (history_size,) to (history_size, 1)\n",
        "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
        "    labels.append(np.reshape(dataset[i:i+target_size], (target_size, 1)))\n",
        "  return np.array(data), np.array(labels)\n",
        "\n",
        "def create_time_steps(length):\n",
        "  return list(range(-length, 0))\n",
        "\n",
        "def show_plot(plot_data, delta, title):\n",
        "  labels = ['History', 'Model Prediction', 'True Future']\n",
        "  marker = ['.-', '.-', '.-']\n",
        "  time_steps = create_time_steps(plot_data[0].shape[0])\n",
        "  if delta:\n",
        "    future = delta\n",
        "  else:\n",
        "    future = 0\n",
        "\n",
        "  plt.title(title)\n",
        "  for i, x in enumerate(plot_data):\n",
        "    if i:\n",
        "      plt.plot(plot_data[i], marker[i], markersize=5,\n",
        "               label=labels[i])\n",
        "    else:\n",
        "      plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
        "  plt.legend()\n",
        "  plt.xlim([time_steps[0], (future+8)*2])\n",
        "  plt.xlabel('Time-Step')\n",
        "  return plt\n",
        "\n",
        "def df_by_store_prophet(df, store_no):\n",
        "  df_store = df[df['Store'] == store_no]\n",
        "  df_store = df_store.rename(columns = {'Date': 'ds', 'Weekly_Sales':'y'})\n",
        "  df_example = df_store.groupby(by = 'ds').agg({'y':'sum'})\n",
        "  df_example.index = pd.to_datetime(df_example.index) \n",
        "  df_example = df_example.asfreq(freq='7D')\n",
        "  df_example = df_example.sort_index(ascending = True)\n",
        "  df_example = df_example.fillna(value=0)\n",
        "  return df_example\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}